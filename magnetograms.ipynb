{
 "metadata": {
  "name": "",
  "signature": "sha256:29738e0a8dda6f45a9fbd5c6026fb3d70064563476088ee9f698a836e0f9a17d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import Image as im\n",
      "import datetime as dt\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "# Images in file\n",
      "basedir = 'data/2010_256/'\n",
      "flist = os.listdir(basedir)\n",
      "num_images = len(flist)\n",
      "\n",
      "# Create mask array to select only within solar disk\n",
      "def make_sun_mask(centre, radius, image_size):\n",
      "    sun_mask = np.zeros([image_size,image_size], dtype=np.uint8)\n",
      "    for i in range(image_size):\n",
      "        for j in range(image_size):\n",
      "            dist = np.sqrt((centre[0] - i)**2 + (centre[1] - j)**2)\n",
      "            if dist<= radius:\n",
      "                sun_mask[i,j] = 1\n",
      "    return sun_mask\n",
      "# Make sun mask\n",
      "image_size = 512\n",
      "centre = (255,255)\n",
      "radius = 472/2\n",
      "\n",
      "image_size = 256\n",
      "centre = (127,127)\n",
      "radius = 234/2\n",
      "\n",
      "sun_mask = make_sun_mask(centre, radius, image_size)\n",
      "# Flatten array\n",
      "sun_mask = np.ravel(sun_mask)\n",
      "# Initialise arrays\n",
      "years = np.zeros(num_images, dtype=np.int)\n",
      "months = np.zeros(num_images, dtype=np.int)\n",
      "days = np.zeros(num_images, dtype=np.int)\n",
      "hours = np.zeros(num_images, dtype=np.int)\n",
      "minutes = np.zeros(num_images, dtype=np.int)\n",
      "sun_discs = np.zeros([num_images,sum(sun_mask)], dtype=np.int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all vector magnetogram files\n",
      "count = 0\n",
      "for image_file in flist:    \n",
      "    years[count] = image_file[0:4]\n",
      "    months[count] = image_file[4:6]\n",
      "    days[count] = image_file[6:8]\n",
      "    hours[count] = image_file[9:11]\n",
      "    minutes[count] = image_file[11:13]\n",
      "    \n",
      "    cur_file = basedir + image_file  \n",
      "    # Read image, convert to greyscale, convert to np array\n",
      "    cur_image = np.asarray(im.open(cur_file).convert('L'))\n",
      "    cur_image = np.ravel(cur_image)\n",
      "    cur_image = cur_image[sun_mask == 1]\n",
      "    sun_discs[count,:] = cur_image\n",
      "    if count%1000 == 0 and count > 0:\n",
      "        print str(count) + ' of ' + str(num_images) +' , File: '+ image_file \n",
      "    count +=1 \n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000 of 57329 , File: 20100511_113000_M_256.jpg\n",
        "2000 of 57329 , File: 20100521_234500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3000 of 57329 , File: 20100601_110000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4000 of 57329 , File: 20100611_211500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5000 of 57329 , File: 20100622_071500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6000 of 57329 , File: 20100702_180000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7000 of 57329 , File: 20100713_051500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8000 of 57329 , File: 20100724_091500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9000 of 57329 , File: 20100803_193000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10000 of 57329 , File: 20100814_054500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11000 of 57329 , File: 20100824_161500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12000 of 57329 , File: 20100904_033000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13000 of 57329 , File: 20100914_144500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14000 of 57329 , File: 20100926_024500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15000 of 57329 , File: 20101008_003000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16000 of 57329 , File: 20101018_131500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17000 of 57329 , File: 20101029_054500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18000 of 57329 , File: 20101109_014500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19000 of 57329 , File: 20101119_121500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20000 of 57329 , File: 20101129_221500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21000 of 57329 , File: 20101210_094500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22000 of 57329 , File: 20101221_104500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23000 of 57329 , File: 20110101_173000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24000 of 57329 , File: 20110112_061500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25000 of 57329 , File: 20110122_233000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26000 of 57329 , File: 20110202_183000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27000 of 57329 , File: 20110213_044500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28000 of 57329 , File: 20110223_154500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29000 of 57329 , File: 20110306_034500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30000 of 57329 , File: 20110316_204500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31000 of 57329 , File: 20110328_011500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32000 of 57329 , File: 20110408_030000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33000 of 57329 , File: 20110418_220000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34000 of 57329 , File: 20110429_143000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35000 of 57329 , File: 20110510_011500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36000 of 57329 , File: 20110520_111500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37000 of 57329 , File: 20110530_221500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38000 of 57329 , File: 20110610_081500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39000 of 57329 , File: 20110620_181500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40000 of 57329 , File: 20110701_053000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41000 of 57329 , File: 20110711_154500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42000 of 57329 , File: 20110722_123000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43000 of 57329 , File: 20110801_234500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44000 of 57329 , File: 20110812_103000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45000 of 57329 , File: 20110822_203000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46000 of 57329 , File: 20110902_083000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47000 of 57329 , File: 20110912_200000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48000 of 57329 , File: 20110923_234500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49000 of 57329 , File: 20111005_113000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50000 of 57329 , File: 20111015_234500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51000 of 57329 , File: 20111026_154500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52000 of 57329 , File: 20111106_064500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53000 of 57329 , File: 20111116_180000_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54000 of 57329 , File: 20111127_041500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55000 of 57329 , File: 20111207_151500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56000 of 57329 , File: 20111218_024500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "57000 of 57329 , File: 20111228_134500_M_256.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#####################################################################################\n",
      "# Import lasco CME catalogue - \n",
      "# Date\n",
      "# Time\n",
      "# Central_PA : Central size [deg] \n",
      "# Width : Angular width [deg]\n",
      "# L_speed : linear speed\n",
      "# 2nd_o_speed : 2nd order speed fit\n",
      "# 2nd_o_speed_20R : 2nd order speed fit @ 20R\n",
      "# Accel : Acceleration\n",
      "# Mass :\n",
      "# KE : Kinetic energy\n",
      "# MPA : Measurement position angle\n",
      "df_cmes = pd.read_csv(\"cme_catalogue.md\",delim_whitespace=True,error_bad_lines =False, warn_bad_lines=False )\n",
      "df_cmes.drop(['Comment1','Comment2', 'Comment3'], axis=1, inplace=True)\n",
      "df_cmes[['year','month','day']] = df_cmes.Date.str.extract('(\\d\\d\\d\\d)/(\\d\\d)/(\\d\\d)')\n",
      "df_cmes[['hour','minute']] = df_cmes.Time.str.extract('(\\d\\d):(\\d\\d)')\n",
      "\n",
      "df_cmes.drop(['Date','Time','KE','Mass','MPA'],axis=1, inplace=True)    \n",
      "df_cmes['CME'] = 1\n",
      "df_cmes['halo'] = df_cmes.Central_PA\n",
      "df_cmes.ix[df_cmes.halo!='Halo', 'halo'] = 0\n",
      "df_cmes.ix[df_cmes.halo=='Halo', 'halo'] = 1\n",
      "df_cmes.ix[df_cmes.Central_PA=='Halo', 'Central_PA'] = 360\n",
      "# Make the features integer\n",
      "for feature in df_cmes.columns:\n",
      "    df_cmes[feature] = df_cmes[feature].astype(int)\n",
      "# Sort all on year -> month->day\n",
      "df_cmes = df_cmes.sort(columns=['year','month','day'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make datetime object in a new columns\n",
      "df_cmes['datetime'] = df_cmes.year\n",
      "for index in range(len(df_cmes)):\n",
      "    df_cmes.ix[index,'datetime'] = dt.datetime(df_cmes.ix[index,'year'],\n",
      "                                               df_cmes.ix[index,'month'],\n",
      "                                               df_cmes.ix[index,'day'],\n",
      "                                               df_cmes.ix[index,'hour'],\n",
      "                                               df_cmes.ix[index,'minute'])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Clip to existing vector magnetogram data\n",
      "df_cmes = df_cmes[(df_cmes.year >= years[0]) & (df_cmes.month >= months[0])]\n",
      "df_cmes = df_cmes[(df_cmes.year <= years[-1]) & (df_cmes.month <= months[-1])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Empty arrays to list indexes of events\n",
      "cmes = np.zeros(num_images, dtype=np.int)\n",
      "halos = np.zeros(num_images, dtype=np.int)\n",
      "\n",
      "# Minutes before CME set here\n",
      "# sec_before_cme = 30*60\n",
      "\n",
      "start = 1\n",
      "for index2 in range(0,len(df_cmes)-1): \n",
      "    cur_row = df_cmes.iloc[index2]\n",
      "    for index in range(start, num_images-1):        \n",
      "        cur_datetime = dt.datetime(years[index],\n",
      "                                   months[index],\n",
      "                                   days[index],\n",
      "                                   hours[index],\n",
      "                                   minutes[index])\n",
      "\n",
      "#         time_diff = (cur_row.datetime - cur_datetime).seconds\n",
      "\n",
      "#         if time_diff <=sec_before_cme:                  \n",
      "#             start = index +1\n",
      "#             cmes[index] = 1\n",
      "#             if cur_row.halo == 1:\n",
      "#                 halos[index] = 1 \n",
      "                \n",
      "        if cur_datetime > cur_row.datetime:\n",
      "            break\n",
      "    cmes[index-1] = 1\n",
      "    if cur_row.halo == 1:\n",
      "        halos[index-1] = 1 \n",
      "        start = index-1\n",
      "        \n",
      "    if index2%100 == 0 and index2 > 0:\n",
      "        print str(index2) + ' of ' + str(len(df_cmes)) \n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 of 1129\n",
        "200 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "300 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "400 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "500 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "600 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "700 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "800 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "900 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1100 of 1129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "quiet = np.zeros(num_images, dtype=np.int)\n",
      "# Length of quiet intervals (approx quiet_thresh*15min)\n",
      "quiet_thresh = 5\n",
      "for index in range(num_images-1):\n",
      "    if cmes[index] == 1:\n",
      "        for index2 in range(index+1, num_images):\n",
      "            if cmes[index2] == 1:\n",
      "                break\n",
      "        if (index2-index)/2 >=quiet_thresh:\n",
      "            quiet[index + (index2-index)/2] = 1\n",
      "  \n",
      "both = np.array(np.ceil(np.divide([cmes + quiet],2.0)),dtype=np.int)\n",
      "both = both.reshape(both.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Number of pre-CME frames: ' + str(np.sum(cmes))\n",
      "print 'Total number of CMEs: ' + str(sum(df_cmes.CME==1))\n",
      "print 'Number of pre-halo frames: ' + str(np.sum(halos))\n",
      "print 'Total number of halos: ' + str(sum(df_cmes.halo==1))\n",
      "print 'Number of quiet frames: ' + str(np.sum(quiet))\n",
      "print 'Number of cme + quiet frames: ' + str(np.sum(both))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of pre-CME frames: 1103\n",
        "Total number of CMEs: 1129\n",
        "Number of pre-halo frames: 40\n",
        "Total number of halos: 40\n",
        "Number of quiet frames: 859\n",
        "Number of cme + quiet frames: 1962\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Un-mask and reshape array for plotting\n",
      "def masked_to_image(masked, sun_mask, image_size):\n",
      "    sun_image = np.zeros([image_size**2])\n",
      "    sun_image[sun_mask==1] = masked\n",
      "    sun_image = sun_image.reshape([image_size, image_size])\n",
      "    return sun_image\n",
      "\n",
      "def plot_gallery(images, titles, h, w):\n",
      "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "    n_components = images.shape[0]\n",
      "    n_col = int(np.sqrt(n_components)) + np.ceil(np.sqrt(n_components)%1)\n",
      "    n_row = int(np.sqrt(n_components)) +np.ceil(np.sqrt(n_components)%1)\n",
      "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
      "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=0.2, wspace=0.2)\n",
      "    for i in range(n_components):\n",
      "        plt.subplot(n_row, n_col, i + 1)\n",
      "        plt.imshow(images[i,:,:], cmap=plt.cm.gray)\n",
      "        plt.title(titles[i], size=14)\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Number of PCA components\n",
      "from sklearn.decomposition import PCA\n",
      "def plot_PCAS(sun_discs, event_array, n_components):\n",
      "    t0 = time()\n",
      "    pca = PCA(n_components=n_components, whiten=False).fit(sun_discs[event_array==1,:])\n",
      "    print(\"done in %0.3fs\" % (time() - t0))\n",
      "    eigensuns = np.zeros([n_components,image_size,image_size])\n",
      "    for n in range(n_components):\n",
      "        eigensuns[n,:,:] = masked_to_image((pca.components_)[n,:], sun_mask, image_size)\n",
      "    eigensuns_titles = [\"Eigensun %d\" % i for i in range(eigensuns.shape[0])]\n",
      "    plot_gallery(eigensuns, eigensuns_titles, image_size, image_size)\n",
      "    plt.show()\n",
      "    \n",
      "n_components = 10\n",
      "plot_PCAS(sun_discs, cmes, n_components)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done in 41.405s\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_X = sun_discs[both==1,:]\n",
      "training_y = cmes[both==1]\n",
      "\n",
      "print (training_X.shape), (training_y.shape)\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X_split1, X_split2, y_split1, y_split2 = train_test_split(training_X, training_y, test_size=0.30, random_state = 30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1962, 42981) (1962,)\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB \n",
      "\n",
      "MNB = MultinomialNB(alpha = 1)\n",
      "MNB.fit(X_split1,y_split1)\n",
      "y_predict1 = MNB.predict(X_split1)\n",
      "y_predict2 = MNB.predict(X_split2)\n",
      "metrics.classification_report(y_split1, y_predict1)  \n",
      "print MNB.score(X_split1,y_split1)\n",
      "print MNB.score(X_split2,y_split2)\n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.554260742899\n",
        "0.519524617997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.49      0.48      0.48       598\n",
        "          1       0.60      0.61      0.61       775\n",
        "\n",
        "avg / total       0.55      0.55      0.55      1373\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.45      0.37      0.40       261\n",
        "          1       0.56      0.64      0.60       328\n",
        "\n",
        "avg / total       0.51      0.52      0.51       589\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier  \n",
      "RF = RandomForestClassifier(min_samples_split=300)\n",
      "\n",
      "\n",
      "RF.fit(X_split1,y_split1)\n",
      "y_predict1 = RF.predict(X_split1)\n",
      "y_predict2 = RF.predict(X_split2)\n",
      "metrics.classification_report(y_split1, y_predict1)  \n",
      "print RF.score(X_split1,y_split1)\n",
      "print RF.score(X_split2,y_split2)\n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)\n",
      "# importance = zip(RF.feature_importances_, features)\n",
      "# for rank in sorted(importance, key=lambda x: x[0], reverse=True):\n",
      "#     print rank\n",
      "# plt.scatter(training_X[:,1], training_y)\n",
      "# predict_all = RF.predict(training_X) \n",
      "# plt.hold\n",
      "# plt.scatter(training_X[:,1],predict_all,color='r')\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.64457392571\n",
        "0.539898132428"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.81      0.24      0.37       598\n",
        "          1       0.62      0.95      0.75       775\n",
        "\n",
        "avg / total       0.70      0.64      0.59      1373\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.41      0.09      0.15       261\n",
        "          1       0.55      0.90      0.68       328\n",
        "\n",
        "avg / total       0.49      0.54      0.45       589\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression  \n",
      "LR = LogisticRegression(penalty ='l1',C=0.001)\n",
      "\n",
      "LR.fit(X_split1,y_split1)\n",
      "y_predict1 = LR.predict(X_split1)\n",
      "y_predict2 = LR.predict(X_split2)\n",
      "metrics.classification_report(y_split1, y_predict1)  \n",
      "print LR.score(X_split1,y_split1)\n",
      "print LR.score(X_split2,y_split2)\n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)\n",
      "# importance = zip(RF.feature_importances_, features)\n",
      "# for rank in sorted(importance, key=lambda x: x[0], reverse=True):\n",
      "#     print rank\n",
      "# plt.scatter(training_X[:,1], training_y)\n",
      "# predict_all = RF.predict(training_X) \n",
      "# plt.hold\n",
      "# plt.scatter(training_X[:,1],predict_all,color='r')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    }
   ],
   "metadata": {}
  }
 ]
}